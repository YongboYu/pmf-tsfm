# LoRA configuration for Moirai models
# Based on KIT-IAI Decision-Focused Fine-Tuning Paper (Table 3)

# LoRA rank - controls adaptation capacity
r: 8

# LoRA alpha - scaling factor (typically 2x rank)
alpha: 16

# LoRA dropout - no regularization as per paper
dropout: 0.0

# Target modules - Moirai attention layers
# Moirai uses standard transformer naming: q_proj, k_proj, v_proj, out_proj
target_modules:
  - q_proj
  - k_proj
  - v_proj
  - out_proj

# Bias handling
bias: none

# Note: These settings match the paper's configuration for optimal performance
# Suitable for Moirai 1.1 (small, large) models
