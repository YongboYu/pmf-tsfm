# LoRA configuration for Chronos Bolt fine-tuning

# LoRA rank - controls adaptation capacity
r: 8

# LoRA alpha - scaling factor (typically 2x rank)
alpha: 16

# LoRA dropout - regularization (0 for no regularization)
dropout: 0.0

# Target modules - T5 attention projections used in Chronos Bolt
target_modules:
  - q
  - v

# Bias handling
bias: none
